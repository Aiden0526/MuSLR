{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1dccbf",
   "metadata": {},
   "source": [
    "# LogiCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, base64\n",
    "from pathlib import Path\n",
    "import mimetypes\n",
    "from PIL import Image, ImageFile\n",
    "import io\n",
    "import sys\n",
    "\n",
    "def load_manifest(path: str):\n",
    "    data = json.load(open(path, \"r\"))\n",
    "    return data if isinstance(data, list) else [data]\n",
    "\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = False\n",
    "\n",
    "def encode_image_base64(image_file: str) -> tuple[str, str]:\n",
    "    img_path = Path(image_file)\n",
    "    print(f\"[DEBUG] Starting encode for: {img_path}\", file=sys.stderr)\n",
    "\n",
    "    try:\n",
    "        raw_bytes = img_path.read_bytes()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not read bytes from {img_path!r}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "    mime_type, _ = mimetypes.guess_type(img_path.name)\n",
    "    if mime_type is None:\n",
    "        raise ValueError(f\"[ERROR] Could not guess MIME type for {img_path!r}\")\n",
    "\n",
    "    if mime_type.lower() in (\"image/jpeg\", \"image/jpg\"):\n",
    "        print(f\"[DEBUG] {img_path} is already JPEG\", file=sys.stderr)\n",
    "        b64 = base64.b64encode(raw_bytes).decode(\"utf-8\")\n",
    "        return b64, \"image/jpeg\"\n",
    "\n",
    "    print(f\"[DEBUG] Converting {img_path} ({mime_type}) → JPEG…\", file=sys.stderr)\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(raw_bytes))\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] PIL.Image.open failed on {img_path!r}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        rgb = img.convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] convert('RGB') failed on {img_path!r}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    try:\n",
    "        rgb.save(buffer, format=\"JPEG\", quality=85, optimize=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] rgb.save() failed for {img_path!r}: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "    jpeg_bytes = buffer.getvalue()\n",
    "    b64 = base64.b64encode(jpeg_bytes).decode(\"utf-8\")\n",
    "    print(f\"[DEBUG] Conversion successful for: {img_path}\", file=sys.stderr)\n",
    "    return b64, \"image/jpeg\"\n",
    "\n",
    "def make_prompt(item: dict) -> str:\n",
    "    ctx = item[\"full_context\"].strip()\n",
    "    q = item[\"question\"].strip()\n",
    "\n",
    "    if item.get(\"type\") == \"multiple_choice\":\n",
    "        choices = \"\\n\".join(f\"- {c}\" for c in item[\"choices\"])\n",
    "        prompt_body = f\"{ctx}\\n\\n{q}\\n\\nChoices:\\n{choices}\"\n",
    "    else:\n",
    "        prompt_body = f\"{ctx}\\n\\n{q}\"\n",
    "    \n",
    "    with open(\"./prompts/logicam/prompts.txt\", \"r\") as f:\n",
    "        prompt_template = f.read()\n",
    "    \n",
    "    prompt_body = prompt_template + prompt_body\n",
    "    \n",
    "    prompt_body += (\n",
    "        \"\\n\\nPlease provide the answer in the following format:\\n\"\n",
    "        \"Reasoning: <your reasoning>\"\n",
    "        \"Answer: <your answer>\\n\"\n",
    "        \"Please put your answer in a curly bracket, e.g., {True}.\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt_body\n",
    "\n",
    "manifest = load_manifest(\"./data/muslr.json\")\n",
    "print(\"Number of instances loaded: \", len(manifest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "OPENAI_MODELS = [\"gpt-4.1-2025-04-14\"]\n",
    "\n",
    "for model in OPENAI_MODELS:\n",
    "\n",
    "    out_path = f\"./logicam_requests_{model}.jsonl\"\n",
    "    counter = 0\n",
    "    with open(out_path, \"w\") as fout:\n",
    "        for item in manifest:\n",
    "            img_uri, _ = encode_image_base64(item[\"image_file\"])\n",
    "            prompt      = make_prompt(item)\n",
    "            full_prompt = f\"{img_uri}\\n\\n{prompt}\"\n",
    "\n",
    "            request = {\n",
    "                \"custom_id\": item[\"id\"],\n",
    "                \"method\":    \"POST\",\n",
    "                \"url\":       \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\":       model,\n",
    "                    \"messages\":   [{\"role\": \"user\", \"content\": full_prompt}],\n",
    "                    \"max_tokens\": 4096,\n",
    "                    \"temperature\": 0.0,\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(request) + \"\\n\")\n",
    "            counter += 1\n",
    "\n",
    "    print(f\"Wrote {counter} requests to {out_path}\")\n",
    "    \n",
    "\n",
    "\n",
    "data = []\n",
    "with open('./logicam/logicam_request.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(f\"Number of requests loaded: {len(data)}\")\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "import json\n",
    "\n",
    "openai.api_key = '<OPENAI_API_KEY>'\n",
    "client = openai.Client(api_key=openai.api_key)\n",
    "\n",
    "def submit_batch(batch_path, desc=\"\"):\n",
    "    path = batch_path\n",
    "    batch_input_file = client.files.create(\n",
    "    file=open(path, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "\n",
    "    print(\"Batch id: \", batch_input_file_id)\n",
    "\n",
    "    batch_request = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "        \"description\": desc\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"Batch request: \", batch_request)\n",
    "\n",
    "def cancel_batch(batch_id):\n",
    "    client.batches.cancel(batch_id)\n",
    "    print(\"Batch cancelled: \", batch_id)\n",
    "\n",
    "def check_batch(batch_id):\n",
    "    batch = client.batches.retrieve(batch_id)\n",
    "    print(\"Batch: \", batch)\n",
    "\n",
    "def retrieve_batch(file_id, save_path):\n",
    "    file_response = client.files.content(file_id)\n",
    "    response_text = file_response.text\n",
    "\n",
    "    responses = [json.loads(line) for line in response_text.strip().split('\\n')]\n",
    "\n",
    "    output_path = save_path\n",
    "    with open(output_path, 'w') as outfile:\n",
    "        json.dump(responses, outfile, indent=2)\n",
    "\n",
    "    print(f\"Responses saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edbf653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # submit batch\n",
    "# logicam_batch = submit_batch(out_path, desc=\"test\")\n",
    "\n",
    "# # check batch status\n",
    "# batch_id = '<BATCH_ID>'\n",
    "# batch_info = check_batch(batch_id)\n",
    "\n",
    "# # retrieve batch results\n",
    "# output_file_id = batch_info.get(\"output_file_id\")\n",
    "# retrieve_batch(output_file_id, './results/logicam_results.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vis_symb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
